---
permalink: /rad/
layout: post
title:  "Reinforcement Learning with Augmented Data"
date:   2020-04-24 11:10:11 -0800
image: /images/rad/rad_thumbnail.png
categories: research
affiliation: UC Berkeley, BAIR
authors: "<b>Michael Laskin*</b>, Kimin Lee*, Adam Stooke, Lerrel Pinto, Pieter Abbeel, Aravind Srinivas"
venue: "NeurIPS <b style='color:red'>Spotlight</b> (top 3% of submissions)"
code: https://github.com/MishaLaskin/rad
arxiv: https://arxiv.org/abs/2004.14990
equal: '*'
press: https://venturebeat.com/2020/05/02/uc-berkeley-researchers-open-source-rad-to-improve-any-reinforcement-learning-algorithm/
twitter: https://twitter.com/MishaLaskin/status/1257001857043529729
summary: First extensive study of image augmentation in the RL setting. Showed that simple RL algorithms with augmented data achieve SOTA results on many common RL benchmarks.
---
<script>
function myFunction() {
  var x = document.getElementById("bibtex");
  if (x.style.display === "none") {
    x.style.display = "block";
  } else {
    x.style.display = "none";
  }
}

</script>
# Reinforcement Learning with <br>Augmented Data
<br> 

<div class="table-like" style="justify-content:space-evenly;max-width:800px;margin:;">
      <div><span class='author_name'><a href="https://mishalaskin.github.io/" target="_blank">Misha Laskin*</a></span>
      <br>
      <span class='author_name'>UC Berkeley</span>
      </div>
      <div><span class='author_name'><a href="https://sites.google.com/view/kiminlee" target="_blank">Kimin Lee*</a></span>
      <br>
      <span class='author_name'>UC Berkeley</span>
      </div>

      <div><span class='author_name'><a href="https://www.linkedin.com/in/adam-stooke-06bb6923/" target="_blank">Adam Stooke</a></span>
      <br>
      <span class='author_name'>UC Berkeley</span>
      </div>
    
      <div><span class='author_name'><a href="https://cs.nyu.edu/~lp91/" target="_blank">Lerrel Pinto</a></span>
      <br>
        <span class='author_name'>UC Berkeley</span>
        </div>

      <div><span class='author_name'><a href="https://people.eecs.berkeley.edu/~pabbeel/" target="_blank">Pieter Abbeel</a></span>
      <br><span class='author_name'>UC Berkeley</span>
      </div>

      <div><span class='author_name'><a href="https://people.eecs.berkeley.edu/~aravind/" target="_blank"> Aravind Srinivas</a></span>
      <br><span class='author_name'>UC Berkeley</span>
      </div>
</div>
<span>* equal contribution</span>

<br>

<p>
  Links ðŸ‘‰ <a   target='_blank' href="{{page.code}}"><u>Github Code</u></a>
        , <a  target='_blank' href="{{page.arxiv}}"><u> ArXiv Paper</u></a>
         , <a  onclick="myFunction()" href="#"><u> Cite BibTex</u></a>
<br>
Media	ðŸ“° <a  target='_blank' href="{{page.press}}"><u> VentureBeat Article</u></a>,
 <a  target="_blank" href="https://twitter.com/MishaLaskin/status/1275595976510758918"><u> Twitter</u></a>, <a  target="_blank" href="https://bair.berkeley.edu/blog/2020/07/19/curl-rad/"><u> BAIR blog </u></a>
</p>

<div style='display:none;font-size:12px' id="bibtex">
  <pre>
@unpublished{laskin_lee2020rad,
  title={Reinforcement Learming with Augmented Data},
  author={Laskin, Michael and Lee, Kimin and Stooke,
  Adam and Pinto, Lerrel and Abbeel, Pieter and Srinivas, Aravind},
  note={arXiv:2004.14990}
}
</pre>
</div>


<br>


## Summary

Learning from visual observations is a fundamental yet challenging problem in reinforcement learning (RL). Although algorithmic advancements combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) <i>sample efficiency of learning</i> and (b) <i>generalization to new environments</i>. 

To this end, we present RAD: Reinforcement Learning with Augmented Data, a simple plug-and-play module that can enhance any RL algorithm. We show that data augmentations such as random crop, color jitter, patch cutout, and random convolutions can enable simple RL algorithms to match and even outperform complex state-of-the-art methods across common benchmarks in terms of data-efficiency, generalization, and wall-clock speed. We find that data diversity alone can make agents focus on meaningful information from high-dimensional observations without any changes to the reinforcement learning method. 

On the DeepMind Control Suite, we show that RAD is state-of-the-art in terms of data-efficiency and performance across 15 environments. We further demonstrate that RAD can significantly improve the test-time generalization on several OpenAI ProcGen benchmarks. Finally, our customized data augmentation modules enable faster wall-clock speed compared to competing RL techniques.

<br>

## Augmentations 

For pixel-based environments, we considered a diverse set of data augmentations and found that random cropping and translation alone enabled a simple RL algorithm (SAC) to achieve state-of-the-art performance, surpassing more complex algorithms. We showed similar results for state-based RL with a new augmentation - <i> random amplitude scaling</i>.

<br>
<center>
<img class='hvr-grow' src="/images/rad/rad_all_augs_overview.png" alt="" style="width:90%;"/>
</center>

<br>

## Results

RAD outperformed previously competing baselines on the DeepMind control suite and, most suprisingly, matched and sometimes outperformed the state-based baseline where the RL agent had access to coordinate state. We also showed state-of-the-art results for generalization in pixel-based environments (ProcGen) and performance state-based RL (OpenAI gym).

<br>
<center>
<img class='hvr-grow' src="/images/rad/fig5.png" alt="" style="width:80%;"/>
</center>
<br>
<hr>
<br>
<center><a  target='_blank' href="{{page.arxiv}}"><u> More details on ArXiv</u></a> </center>
<br>
<center>
<a  target='_blank' href="{{page.arxiv}}">
<img class='hvr-grow' src="/images/rad/rad_paper_thumbnail.png" alt="" style="width:80%;"/>
</a>
</center>

<br>
<hr>
<br>