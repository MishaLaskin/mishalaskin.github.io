---
permalink: /local/
layout: post
title:  "Parallel Training of Deep Networks with Local Updates"
date:   2021-05-02 11:10:11 -0800
image: /images/local/local.gif
categories: research
affiliation: UC Berkeley, Graphcore, Google Brain
authors: "<b>Michael Laskin</b>*, Luke Metz*, Seth Nabarrao, Mark Saroufim, Badreddine Noune, Carlo Luschi, Jascha Sohl-Dickstein, Pieter Abbeel"
venue: "Pre-Print"
arxiv: https://arxiv.org/abs/2012.03837
equal: '*'
twitter: https://twitter.com/MishaLaskin/status/1336401401350598659
summary:  Local learning, an alternative to global backpropagation, improves the efficiency of training deep nets in the high-compute regime. 

---
<script>
function myFunction() {
  var x = document.getElementById("bibtex");
  if (x.style.display === "none") {
    x.style.display = "block";
  } else {
    x.style.display = "none";
  }
}

</script>
# Parallel Training of Deep Networks with Local Updates
<br> 

<div class="table-like" style="justify-content:space-evenly;max-width:800px;margin:;">

      <div><span class='author_name'><a href="https://mishalaskin.github.io/" target="_blank">Misha Laskin</a></span>
      <br>
      <span class='author_name'>UC Berkeley</span>
      </div>
    
      <div><span class='author_name'><a href="https://sites.google.com/view/kiminlee" target="_blank">Luke Metz</a></span>
      <br>
      <span class='author_name'>Google</span>
      </div>

      <div><span class='author_name'><a href="https://people.eecs.berkeley.edu/~pabbeel/" target="_blank">Seth Nabarrao</a></span>
      <br><span class='author_name'>Graphcore</span>
      </div>

      <div><span class='author_name'><a href="https://people.eecs.berkeley.edu/~pabbeel/" target="_blank"> Mark Saroufim </a></span>
      <br><span class='author_name'>Graphcore</span>
      </div>

      <div><span class='author_name'><a href="https://people.eecs.berkeley.edu/~pabbeel/" target="_blank">Badreddine Noune</a></span>
      <br><span class='author_name'>Graphcore</span>
      </div>

      <div><span class='author_name'><a href="https://people.eecs.berkeley.edu/~pabbeel/" target="_blank"> Carlo Luschi</a></span>
      <br><span class='author_name'>Graphcore</span>
      </div>

      <div><span class='author_name'><a href="https://people.eecs.berkeley.edu/~pabbeel/" target="_blank">Jascha Sohl-Dickstein</a></span>
      <br><span class='author_name'>Google</span>
      </div>

      <div><span class='author_name'><a href="https://people.eecs.berkeley.edu/~pabbeel/" target="_blank">Pieter Abbeel</a></span>
      <br><span class='author_name'>UC Berkeley</span>
      </div>
</div>

<br>

<p>
  Links ðŸ‘‰ 
         <a  target='_blank' href="{{page.arxiv}}"><u> ArXiv Paper</u></a>
         , <a  onclick="myFunction()" href="#"><u> Cite BibTex</u></a>
<br>
Media	ðŸ“°
 <a  target="_blank" href="https://twitter.com/MishaLaskin/status/1306779645287763970"><u> Twitter</u></a>
</p>

<div style='display:none;font-size:12px' id="bibtex">
  <pre>
@misc{laskin_metz2020local,
Author = {Michael Laskin and Luke Metz and Seth Nabarrao and Mark Saroufim and Badreddine Noune and Carlo Luschi and Jascha Sohl-Dickstein and Pieter Abbeel},
Title = {Parallel Training of Deep Networks with Local Updates},
Year = {2020},
Eprint = {arXiv:2012.03837},
}
</pre>
</div>


<br>


## Abstract 

Deep learning models trained on large data sets have been widely successful in both vision and language domains. As state-of-the-art deep learning architectures have continued to grow in parameter count so have the compute budgets and times required to train them, increasing the need for compute-efficient methods that parallelize training. Two common approaches to parallelize the training of deep networks have been data and model parallelism. While useful, data and model parallelism suffer from diminishing returns in terms of compute efficiency for large batch sizes. In this paper, we investigate how to continue scaling compute efficiently beyond the point of diminishing returns for large batches through local parallelism, a framework which parallelizes training of individual layers in deep networks by replacing global backpropagation with truncated layer-wise backpropagation. Local parallelism enables fully asynchronous layer-wise parallelism with a low memory footprint, and requires little communication overhead compared with model parallelism. We show results in both vision and language domains across a diverse set of architectures, and find that local parallelism is particularly effective in the high-compute regime.
<br>
