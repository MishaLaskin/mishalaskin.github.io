---
permalink: /sgm/
layout: post
title:  "Sparse Graphical Memory for Robust Planning"
date:   2020-03-06 11:10:11 -0800
image: /images/cleanup.gif
categories: research
affiliation: UC Berkeley
authors: "<b>Michael Laskin*</b>, Scott Emmons*, Ajay Jain*, Thanard Kurutach, Pieter Abbeel, Deepak Pathak"
venue: "In Submission"
code: https://github.com/scottemmons/sgm
arxiv: https://drive.google.com/file/d/1-Us-ZxQKjdF5_Wbqspef5uxAydxCtYxG/view
---
Sparse Graphical Memory (SGM) combines deep RL and classical planning to solve long-horizon tasks from images. We introduce a two-way consistency check that enables dynamic build out of a graph over observations and show significant improvements over existing graphical methods in RL.

<!--<div style="text-align:center"><img style="width:100%;max-width:20%" src="/images/cleanup_obs.gif1" /></div>-->

To operate effectively in the real world, artificial agents must act from raw sensory input such as images and achieve diverse goals across long time-horizons. On the one hand, recent strides in deep reinforcement and imitation learning have demonstrated impressive ability to learn goal-conditioned policies from high-dimensional image input, though only for short-horizon tasks. On the other hand, classical graphical methods like A* search are able to solve long-horizon tasks, but assume that the graph structure is abstracted away from raw sensory input and can only be constructed with task-specific priors. We wish to combine the strengths of deep learning and classical planning to solve long-horizon tasks from raw sensory input. To this end, we introduce Sparse Graphical Memory (SGM), a new data structure that stores observations and feasible transitions in a sparse memory. SGM can be combined with goal-conditioned RL or imitative agents to solve long-horizon tasks across a diverse set of domains. We show that SGM significantly outperforms current state of the art methods on long-horizon, sparse-reward visual navigation tasks.